---
title: "LSH para categorías de Wikipedia en Spark"
output: html_notebook
---


## Cargar datos en cluster

Normalmente, este paso no lo hacemos en nuestra sesión de análisis: los datos
están distribuidos en un cluster originalmente. Para nuestro ejemplo, limpiamos
y cargamos los datos en memoria:

```{r}
library(tidyverse)
limpiar <- function(lineas,...){
  df_lista <- str_split(lineas, " ") %>% 
    keep(function(x) x[1] != '#') %>%
    transpose %>%
    map(function(col) as.character(col)) 
  df <- data_frame(articulo = df_lista[[1]], 
                   categorias = df_lista[[2]]) 
  df
}
filtrado <- read_lines_chunked("../../datos/similitud/wiki-100000.txt",
                    skip = 1, callback = ListCallback$new(limpiar))
```

Los datos completos están [aquí](https://s3.amazonaws.com/wiki-large/article_categories_en.ttl)

Consideramos los datos ya tokenizados (los tokens son las categorías):

```{r}
articulos_df <- filtrado %>% bind_rows
articulos_df
```


Y registramos en el cluster (en este caso, corremos los scripts localmente):

```{r}
library(sparklyr)
config <- spark_config()
# configuración para modo local:
config$`sparklyr.shell.driver-memory` <- "2G" # para poder hacer collect de pares más adelante
sc <- spark_connect(master = "local", config = config)
# normalmente no copiamos de nuestra sesión de R a un cluster! Para este ejemplo
# con datos chicos es posible:
articulos_wiki_tbl <- copy_to(sc, articulos_df, "articulos_wiki", overwrite = TRUE) 
articulos_wiki_tbl
```

Agrupamos los tokens en una lista:

```{r}
art_agr <- articulos_wiki_tbl %>%
        group_by(articulo) %>%
        summarise(lista = collect_list(categorias)) 
```

Y binarizamos (la representación para usar la implementación de spark es
de matriz rala: 1 cuando el token/shingle pertenece al documento, y 0 si no):

```{r}
art_bin <- art_agr %>% 
        ft_count_vectorizer('lista', 'vector', binary = TRUE) 
```

```{r}
# estimator
lsh_wiki_estimator <- ft_minhash_lsh(sc, 'vector', 'hashes', 
                           seed = 1227,
                           num_hash_tables = 5)
```

```{r}
lsh_wiki_trans <-  ml_fit(lsh_wiki_estimator, art_bin)
art_bin <- ml_transform(lsh_wiki_trans, art_bin)
art_bin %>% head(5)
```

Podemos encontrar vecinos cercanos
```{r}
vec_1 <- art_bin %>% filter(articulo =='Alabama') %>% pull(vector)
similares <- ml_approx_nearest_neighbors(lsh_wiki_trans, 
              art_bin, vec_1[[1]], num_nearest_neighbors = 10) %>% 
              select(articulo, lista, distCol)
print(similares %>% collect)
```

Encontramos pares similares con un *similarity join*, por ejemplo:

```{r}
art_bin <- art_bin %>% mutate(id = articulo)
pares_candidatos <- ml_approx_similarity_join(lsh_wiki_trans, art_bin, art_bin, 0.7,
  dist_col = "distCol") %>% filter(id_a != id_b)
pares_candidatos  %>% tally()
```

```{r}
pares <- pares_candidatos %>% filter(distCol < 0.2)
pares %>% tally
pares <- pares %>% collect()
```

Por ejemplo

```{r}
DT::datatable(pares %>% filter(str_detect(id_a, "poker") | str_detect(id_b, "poker")))
```



Nota: la implementación en spark de LSH utiliza solamente amplificación OR. 
Es posible usar suficientes hashes para obtener pares, y después filtrar
los de la distancia que buscamos (¿Cómo implementar familias AND-OR)?

